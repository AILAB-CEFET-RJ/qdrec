{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing.py\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "def remove_dash_n(text):\n",
    "    return text.replace('/n', ' ')\n",
    "\n",
    "def remove_spaces(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def remove_lots_of_points(text):\n",
    "    return re.sub('\\.{2,}', ' ', text)\n",
    "\n",
    "def remove_bad_chars(text):\n",
    "    return re.sub('[˜˚˝˙ˆˇ˚˘˘Œ˛œ_%ﬁﬂ‡š<>›„’]', ' ', text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text = re.sub('[\\(\\)\\[\\]\\{\\}]', ' ', text)\n",
    "    text = re.sub('[ \\.,]((um|dois|três|quatro|cinco|seis|sete|oito|nove|dez|onze|doze|treze|quatorze|quinze|dezesseis|dezessete|dezoito|desenove|vinte|trinta|quarenta|cinquenta|cincoenta|cinqüenta|sessenta|oitenta|noventa|cem|duzentos|trezentos|quatrocentos|quinhentos|seiscentos|setecentos|oitocentos|novecentos|mil|milhão|bilhão|trilhão)[e \\.,]*)+(?<=[ \\.,])', ' 1', text)\n",
    "    text = re.sub('\\(?R *\\$ *[0-9\\.,]*\\)?', ' valores ', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    text = re.sub('[Nn§]? *[§º°]?\\.? *[0-9]+([nN§º°\\-0-9\\.\\,\\/\\\\ ]|página)*[º°]?', ' número ', text)\n",
    "    text = re.sub('[ \\.,]( |,|e|número|reais|centavos|valores)*(reais|centavos)( |,|e|número|reais|centavos|valores)*[ \\.,]', ' valores ', text)\n",
    "    #Esse está removendo ponto final após o número. Seria bom que não removesse. Mas preciso remover no meio.\n",
    "    text = re.sub('[ \\.,](dias|número|e| )* *(dias)? *(d[oe])? *(mês)? *(de)? *(janeiro|fevereiro|março|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro) *(do)? *(ano)? *(de)? *(número)?', ' data ', text)\n",
    "    text = re.sub('[ ,.](?=[CLXVI])(C{0,3})(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})[ ,.]', \n",
    "                  ' número romano ', text)\n",
    "    text = re.sub('[ ,.](?=[clxvi])(c{0,3})(x[cl]|l?x{0,3})(i[xv]|v?i{0,3})[ ,.]', \n",
    "                  ' número romano ', text)\n",
    "    text = re.sub('(http)[:\\/w]*\\.', 'http ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('(\\.com)(\\.br)?', ' com ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('[ \\.](número) *(h|horas)( |,|número|minutos|m|min)*[ \\.]', ' horas ', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    #text = BeautifulSoup(text, 'html.parser').text\n",
    "    return text\n",
    "\n",
    "def spaced_letters(text):\n",
    "    text = text.replace('A P O S E N T A R', ' aposentar ')\n",
    "    return text\n",
    "\n",
    "def join_words(text):\n",
    "    # Há muitas palavras assim: \"res - ponsável\"\n",
    "    # Vou ter algum problema juntando coisa que não devia\n",
    "    # Espero que nesses casos o tokenizer resolva\n",
    "    return re.sub(' - ?', '', text)\n",
    "\n",
    "def separate_words(text):\n",
    "    # Há muitas palavras assim: \"FerrazPresidente\" \"FerrazAPresidente\"\n",
    "    # Vou supor que sempre que houver uma letra minúscula seguida de maiúscula é para separar\n",
    "    text = re.sub('(?<=[a-záàâãéêíóôõú])(?=[A-ZÁÀÂÃÉÊÍÓÔÕÚ])', ' ', text)\n",
    "    text = re.sub('(?<=[ÓA-ZÁÀÂÃÉÊÍÓÔÕÚ])(?=[ÓA-ZÁÀÂÃÉÊÍÓÔÕÚ][a-záàâãéêíóôõú])', ' ', text)\n",
    "    return text\n",
    "\n",
    "def dots_that_mess_segmentation(text):\n",
    "    text = re.sub('sec\\.', 'Sec ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('av\\.', 'Avenida ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('min\\.', 'Ministro ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('exmo\\.', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('sr\\.', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('dr\\.', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('sra\\.', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('proc\\.', ' processo ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('reg\\.', ' registro ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('func\\.', ' funcionário ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('art\\.', ' artigo ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('inc\\.', ' inciso ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('(?<=[ \\.][A-Z])\\.', ' ', text)\n",
    "    text = re.sub('(?<=comp)\\.', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('(?<=insc)\\.', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('p[áa]g[\\. ]', ' página ', text, flags=re.IGNORECASE) #Pág. N 5\n",
    "    text = re.sub('\\. *(n|n[uú]mero)[ \\.°º]+', ' número ', text, flags=re.IGNORECASE) #. N°\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_dash_n(text)\n",
    "    text = remove_lots_of_points(text)\n",
    "    text = remove_bad_chars(text)    \n",
    "    text = spaced_letters(text)\n",
    "    text = dots_that_mess_segmentation(text)\n",
    "    text = remove_spaces(text)\n",
    "    text = join_words(text)\n",
    "    text = separate_words(text)\n",
    "    text = remove_page_breaker(text)\n",
    "    text = remove_special_characters(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def replaces(text):\n",
    "    #tokenizador separa essas palavras em 3 tokens\n",
    "    text = re.sub('teresina', 'cidade', text) \n",
    "    text = re.sub('piauí', 'estado', text)\n",
    "    text = re.sub('c/c', 'concomitante', text)\n",
    "    return text\n",
    "\n",
    "def posprocess(text):\n",
    "#    text = remove_page_breaker(text)\n",
    "    text = text.lower()\n",
    "    text = remove_numbers(text)\n",
    "    text = replaces(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_special_characters(text:str):\n",
    "    text = text.replace(\"<__\", \"\").replace(\"__>\", \"\")\n",
    "    text = text.replace(\"- -\", \"-\")\n",
    "    text = text.replace(\" - \", \"-\")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    text = text.replace(\"- \", \"-\")\n",
    "    text = text.replace(\" -\", \"-\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_page_breaker(text:str) -> str:\n",
    "    return text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "def find_occurrences(text:str, \n",
    "                     character:str) -> list:\n",
    "    return [i for i, letter in enumerate(text) if letter == character]\n",
    "\n",
    "def get_whole_words(subtext:str) -> str:\n",
    "    spaces_indexes = find_occurrences(subtext, \" \")\n",
    "    first_pos=spaces_indexes[0]\n",
    "    last_pos=spaces_indexes[-1]\n",
    "    \n",
    "    #pega entre o primeiro e o ultimo espaço\n",
    "    #para não pegar palavras pela metade\n",
    "    return (subtext[first_pos:\n",
    "                    last_pos],\n",
    "            first_pos,\n",
    "            last_pos)\n",
    "\n",
    "def contains_number(word):\n",
    "    return bool(re.search(r'\\d', word))\n",
    "\n",
    "\n",
    "def find_dashes_and_replace_words(text:str,\n",
    "                                  df_ptbr:pd.DataFrame) -> str:\n",
    "    dashes_indexes = find_occurrences(text, \"-\")\n",
    "    spaces_indexes = find_occurrences(text, \" \")\n",
    "#    words = df_ptbr['Word'].map(lambda w: unidecode(w.lower())).unique()#unidecode\n",
    "    words = df_ptbr['Word'].map(lambda w: w.lower()).unique()#unidecode\n",
    "\n",
    "    for dash in dashes_indexes:\n",
    "        try:\n",
    "            space_before=max([elem for elem in spaces_indexes if elem < dash])\n",
    "        except:\n",
    "            space_before=0\n",
    "        \n",
    "        try:\n",
    "            space_after= min([elem for elem in spaces_indexes if elem > dash])\n",
    "        except:\n",
    "            space_after=len(text)\n",
    "\n",
    "        new_word=text[space_before:space_after]\n",
    "        new_word=new_word.replace('-', '')\n",
    "\n",
    "#        new_word_cleaned = unidecode(new_word.\n",
    "        new_word_cleaned = (new_word.\n",
    "                                    lower().\n",
    "                                    strip().\n",
    "                                    replace('.', '').\n",
    "                                    replace(',', '').\n",
    "                                    replace(':', '').\n",
    "                                    replace(';', '').\n",
    "                                    replace(')', '').\n",
    "                                    replace('(', '').\n",
    "                                    replace('[', '').\n",
    "                                    replace(']', ''))\n",
    "        if not(contains_number(word=new_word)):        \n",
    "            if new_word_cleaned in words:\n",
    "                text = ''.join([text[:space_before], \n",
    "                                new_word,\n",
    "                                text[space_after:]])\n",
    "\n",
    "        \n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "def clean_text(text:str,\n",
    "               window_size:int=50,\n",
    "               time_between_queries:int=3) -> str:\n",
    "    inicio = datetime.datetime.now()#.strftime(\"%Y%m%d%H:%M:%S\")\n",
    "    logging.info(f\"TEXTO INICIAL {inicio.strftime('%Y%m%d%H:%M:%S')} -> {text}\")\n",
    "    dash_indexes = find_occurrences(text, \"-\")\n",
    "    if dash_indexes:\n",
    "        final_text=''\n",
    "        dash_indexes_size = len(dash_indexes)\n",
    "        for i in range(0, dash_indexes_size):\n",
    "\n",
    "            start_dash_position = dash_indexes[i]-window_size\n",
    "            end_dash_position = dash_indexes[i]+window_size\n",
    "\n",
    "            if start_dash_position < 0:\n",
    "                start_dash_position=0 #pegar inicio do texto caso o intervalo de contexto esteja antes da posição 0\n",
    "\n",
    "            if i==0:\n",
    "                last_position=0\n",
    "                start_dash_position=0\n",
    "            else:\n",
    "                last_position=dash_indexes[i]\n",
    "                if start_dash_position < (dash_indexes[i-1]+window_size):\n",
    "                    start_dash_position= last_space_position#dash_indexes[i-1]+window_size\n",
    "\n",
    "            subtext = text[start_dash_position:\n",
    "                           end_dash_position]\n",
    "\n",
    "            subtext, first_space_position, last_space_position = get_whole_words(subtext=subtext)\n",
    "\n",
    "            first_space_position+=start_dash_position#(last_position)\n",
    "            last_space_position+=start_dash_position#last_position\n",
    "\n",
    "            subtext = fix_spelling_in_answer(subtext)[0] #aqui entra a validação no google\n",
    "\n",
    "            first_fragment = text[start_dash_position:\n",
    "                                  first_space_position]\n",
    "\n",
    "            if i==(dash_indexes_size-1):\n",
    "                last_fragment = text[last_space_position:]\n",
    "            else:\n",
    "                next_dash_position = dash_indexes[i+1]-window_size\n",
    "\n",
    "                last_fragment = text[last_space_position:\n",
    "                                     next_dash_position]\n",
    "\n",
    "            final_text += \" \".join([first_fragment,\n",
    "                                    subtext,\n",
    "                                    last_fragment])\n",
    "            final_text = final_text.replace(\"  \", \" \")\n",
    "    else:\n",
    "        final_text=text\n",
    "        \n",
    "    final = datetime.datetime.now()\n",
    "    logging.info(f\"TEXTO FINAL {final.strftime('%Y%m%d%H:%M:%S')} -> {final_text}\")\n",
    "    logging.info(f\"TEMPO DE PROCESSAMENTO -> {(final-inicio).total_seconds()}\")\n",
    "    \n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtendo palavras do portugues brasileiro\n",
    "url = 'https://drive.google.com/file/d/1tUDeEyH6vonx-ctxeGVWG6gh4knP1Igi/view?usp=sharing'\n",
    "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
    "header_list = ['Word']\n",
    "df_ptbr = pd.read_csv(path, \n",
    "                      skiprows=0,\n",
    "                      names=header_list,\n",
    "                      header=None,\n",
    "                      sep=',')\n",
    " \n",
    "#Pendente: localizar a função que implementei tratando o \"- \" e validando no 'ptbr'."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text='''disso, certifi ca-se que, por ocasião da abertura do processo licitatório, não se antevia o delongamento do período pandêmico, que culminou no dia 29 de\n",
    "dezembro de 2020, na prorrogação do prazo do Estado de Calamidade Pública por mais 180 dias por meio do Decreto nº 1.975. Outrossim, as especifi cações dos referi- dos kits, correspondia à realidade das atividades letivas presenciais e ainda se apresentava dividido em 16 itens, o que não se mostra factível diante do cenário atual. Pelo exposto, a'''\n",
    "\n",
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/pcdas/balthapaixao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import random\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def send_query(query):\n",
    "    while True:\n",
    "        url = \"https://www.google.com.br/search?q={}\".format(query)\n",
    "\n",
    "        headers = {'User-agent': 'your bot 0.1'}\n",
    "\n",
    "        html = requests.get(url, headers=headers)\n",
    "        html = requests.get(url)\n",
    "\n",
    "        if html.status_code == 200:  # Everything is OK\n",
    "            soup = BeautifulSoup(html.text, 'lxml')\n",
    "\n",
    "            a = soup.find(\"a\", {\"id\": \"scl\"})\n",
    "\n",
    "            if a == None:\n",
    "                break\n",
    "\n",
    "            query = a.text\n",
    "\n",
    "        elif html.status_code == 429:  # Too many requests\n",
    "            #print(\"Time to wait:\")\n",
    "            #print(html.headers)\n",
    "            break\n",
    "        else:\n",
    "            #print(\"Error: \", html.status_code)\n",
    "            #print(html)\n",
    "            break\n",
    "\n",
    "    return query, html.status_code\n",
    "\n",
    "\n",
    "def fix_spelling_in_answer(answer):\n",
    "    new_answer, status_code = send_query(answer)\n",
    "    if status_code == 429:\n",
    "        #print(\"429\")\n",
    "        time_to_sleep=random.randint(25, 35)\n",
    "        time.sleep(time_to_sleep)\n",
    "        new_answer, status_code = send_query(answer)\n",
    "\n",
    "    return new_answer, status_code\n",
    "\n",
    "\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/portuguese.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset-tecnologias-educacao.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 16s, sys: 9.46 s, total: 4min 26s\n",
      "Wall time: 4min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['cleaned_text'] = df['excerpt'].map(preprocess)\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda txt:\n",
    "                                         find_dashes_and_replace_words(txt, \n",
    "                                                                       df_ptbr)\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now().strftime(\"%d%m%Y%H:%M\")\n",
    "\n",
    "logging.basicConfig(filename=f'QD-scrap_google-{date}.log',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1355\n"
     ]
    }
   ],
   "source": [
    "unique_text_list = df['cleaned_text'].unique()\n",
    "print(len(unique_text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_save(text_list:list) -> None:\n",
    "    with open(\"de-para.csv\", \"w\",encoding = 'utf-8') as f:\n",
    "        for text in text_list:\n",
    "            new_text = clean_text(text)\n",
    "            f.write(f'{text}\\t{new_text}\\n')\n",
    "    print(\"DONE!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_and_save(unique_text_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
